{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ba835-6e41-4be0-834b-616aff0b9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \".json\"\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview import rag\n",
    "\n",
    "vertexai.init(project=\"proyecto\", location=\"us-east4\")\n",
    "print(list(rag.list_corpora()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2038ddc-974f-4a20-a3d9-fece09784b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c455079-3d17-4be0-b683-8a563b73c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Clasificaci√≥n de sentencias (resultado & parte demandada) con Vertex AI\n",
    "# - Lee .txt desde GCS bucket: \n",
    "# - Guarda CSVs en GCS bucket: \n",
    "# - Reusa UI, logging y helpers\n",
    "# - Crea: gs:///sentencias_resultado_vertex.csv (nombre, demandado, resultado)\n",
    "# - Fusiona con gs:///sentencias_motivos_vertex.csv (si existe) en gs:///sentencias_merged_vertex.csv\n",
    "# =========================================================\n",
    "import os, re, json, time, csv, unicodedata, textwrap, tempfile, io\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# ---- Google Cloud ----\n",
    "from google.cloud import storage\n",
    "\n",
    "# ---- Vertex AI ----\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "PROJECT_ID = \"proyectid\"\n",
    "LOCATION   = \"us-east4\"\n",
    "MODEL_ID   = \"gemini-2.0-flash\"      # o \"gemini-2.0-pro\"\n",
    "\n",
    "# Buckets y nombres de objetos\n",
    "SRC_BUCKET = \"bucket\"   # de aqu√≠ vienen los .txt\n",
    "DST_BUCKET = \"bucket2\"        # aqu√≠ guardamos los CSVs\n",
    "\n",
    "GCS_MOTIVOS_IN = \"sentencias_motivos_vertex.csv\"         # si existe en curated5896, se usa para merge\n",
    "GCS_OUT_RES    = \"sentencias_resultado_vertex.csv\"       # salida\n",
    "GCS_OUT_MERGED = \"sentencias_merged_vertex.csv\"          # salida fusionada\n",
    "\n",
    "# Log local (opcional)\n",
    "LOG_PATH       = \"/dataset/log_vertex.txt\"\n",
    "\n",
    "# Cat√°logo fijo (del script original)\n",
    "MOTIVOS = [\n",
    "    \"acreditaci√≥n de la deuda\",\n",
    "    \"usura\",\n",
    "    \"abusividad del clausulado\",\n",
    "    \"transparencia\",\n",
    "    \"legitimaci√≥n activa\",\n",
    "    \"prescripci√≥n\",\n",
    "    \"validez de la firma\",\n",
    "    \"legitimaci√≥n pasiva\",\n",
    "    \"requerimiento previo derecho al honor\",\n",
    "]\n",
    "\n",
    "# UI\n",
    "VERBOSE_UI = True\n",
    "UI_WRAP = 110\n",
    "RAW_UI_MAX = 2500  \n",
    "\n",
    "# -------------------- UI din√°mica --------------------\n",
    "def _bar(p: float, width: int = 28) -> str:\n",
    "    p = min(max(p, 0.0), 1.0)\n",
    "    filled = int(round(p * width))\n",
    "    return \"‚ñà\" * filled + \"‚ñë\" * (width - filled)\n",
    "\n",
    "class LiveUI:\n",
    "    def __init__(self, enabled=True): self.enabled = enabled\n",
    "    def _truncate(self, s, max_chars=1200): s = s or \"\";  return (s[:max_chars] + \" ‚Ä¶[+trunc]\") if len(s) > max_chars else s\n",
    "    def show(self, archivo, fase, detalle=None, ultimo_json=None, ultimo_raw=None,\n",
    "             progress=None):\n",
    "        if not self.enabled: return\n",
    "        clear_output(wait=True)\n",
    "        lines = []\n",
    "        lines.append(f\"**üìÑ Archivo:** `{archivo}`\")\n",
    "        lines.append(f\"**‚öôÔ∏è Fase:** {fase}\")\n",
    "        if progress is not None:\n",
    "            percent = int(progress * 100)\n",
    "            lines.append(f\"**Progreso:** `{percent:>3d}%`  `{_bar(progress)}`\")\n",
    "        if detalle:\n",
    "            lines.append(f\"**‚ÑπÔ∏è** {detalle}\")\n",
    "        if ultimo_json:\n",
    "            wrapped = \"\\n\".join(textwrap.wrap(self._truncate(ultimo_json, 2000), width=UI_WRAP))\n",
    "            lines.append(\"**üß† Respuesta (JSON limpio):**\\n\\n```json\\n\" + wrapped + \"\\n```\")\n",
    "        if ultimo_raw is not None:\n",
    "            show = ultimo_raw if RAW_UI_MAX is None or len(ultimo_raw) <= RAW_UI_MAX else (ultimo_raw[:RAW_UI_MAX] + \" ‚Ä¶[+trunc]\")\n",
    "            lines.append(\"**üìú Respuesta LITERAL del modelo:**\\n\\n```\\n\" + show + \"\\n```\")\n",
    "        display(Markdown(\"\\n\\n\".join(lines)))\n",
    "\n",
    "ui = LiveUI(enabled=VERBOSE_UI)\n",
    "\n",
    "# -------------------- Utilidades --------------------\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\\n\", \"\\n\", re.sub(r\"[ \\t]{2,}\", \" \", s or \"\")).strip()\n",
    "\n",
    "# -------------------- CSV + LOG --------------------\n",
    "def log_block(path, header, prompt_full, response_full):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{ts} | {header}\\n\")\n",
    "        f.write(\"--- PROMPT START ---\\n\")\n",
    "        f.write((prompt_full or \"\") + \"\\n\")\n",
    "        f.write(\"--- PROMPT END ---\\n\")\n",
    "        f.write(\"--- RESPONSE START ---\\n\")\n",
    "        f.write((response_full or \"\") + \"\\n\")\n",
    "        f.write(\"--- RESPONSE END ---\\n\")\n",
    "\n",
    "# -------------------- GCS helpers --------------------\n",
    "_storage_client = storage.Client()\n",
    "\n",
    "def gcs_list_txt(bucket_name: str, prefix: Optional[str] = None) -> List[str]:\n",
    "    \"\"\"Lista blobs .txt en un bucket opcionalmente bajo un prefijo.\"\"\"\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blobs = _storage_client.list_blobs(bucket, prefix=prefix)\n",
    "    names = [b.name for b in blobs if b.name.lower().endswith(\".txt\")]\n",
    "    names.sort()\n",
    "    return names\n",
    "\n",
    "def gcs_read_text(bucket_name: str, blob_name: str, encoding: str = \"utf-8\") -> str:\n",
    "    \"\"\"Descarga un blob de texto como string.\"\"\"\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    return blob.download_as_text(encoding=encoding)\n",
    "\n",
    "def gcs_blob_exists(bucket_name: str, blob_name: str) -> bool:\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    return blob.exists()\n",
    "\n",
    "def gcs_upload_bytes(bucket_name: str, blob_name: str, data: bytes, content_type: str = \"text/plain\"):\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_string(data, content_type=content_type)\n",
    "\n",
    "def gcs_upload_file(bucket_name: str, blob_name: str, local_path: str, content_type: Optional[str] = None):\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(local_path, content_type=content_type)\n",
    "\n",
    "def gcs_download_to_temp(bucket_name: str, blob_name: str) -> str:\n",
    "    \"\"\"Descarga un blob a un fichero temporal y devuelve su ruta.\"\"\"\n",
    "    bucket = _storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    _, tmp_path = tempfile.mkstemp(prefix=\"gcs_\", suffix=os.path.splitext(blob_name)[1])\n",
    "    blob.download_to_filename(tmp_path)\n",
    "    return tmp_path\n",
    "\n",
    "# -------------------- Prompt + Schema (RESULTADO/DEMANDADO) --------------------\n",
    "def build_prompt_outcome(texto: str) -> str:\n",
    "    return f\"\"\"Eres un analista jur√≠dico estricto.\n",
    "Lee la SENTENCIA COMPLETA en espa√±ol y devuelve EXCLUSIVAMENTE un JSON con DOS claves:\n",
    "- \"demandado\": uno de [\"xxx\",\"contrario\"] {{\"xxx\" si xxx (o xxx) es la parte DEMANDADA; \"contrario\" si la parte demandada es otra distinta a xxx}}\n",
    "- \"resultado\": uno de [\"favorable\",\"desfavorable\"] {{\"favorable\" si el FALLO es favorable a xxx; \"desfavorable\" si es favorable a la otra parte}}\n",
    "\n",
    "Reglas:\n",
    "- Responde en JSON V√ÅLIDO (RFC 8259) y NADA M√ÅS.\n",
    "- Si el texto no identifica con claridad a la parte demandada o el fallo, infiere con criterio jur√≠dico (fundamentos y fallo).\n",
    "- Si mencionan \"Se estima la demanda de xxx\" ‚Üí resultado=\"favorable\".\n",
    "- Si \"Se desestima la demanda de xxx\" ‚Üí resultado=\"desfavorable\".\n",
    "- Si xxx es actor y gana, el fallo tambi√©n es favorable a xxx.\n",
    "- Si xxx es demandado y se desestima la demanda del actor, tambi√©n es favorable a xxx.\n",
    "\n",
    "Ejemplo de salida:\n",
    "{{\"demandado\":\"xxx\",\"resultado\":\"favorable\"}}\n",
    "\n",
    "TEXTO COMPLETO:\n",
    "\\\"\\\"\\\"{normalize_ws(texto)}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "RESPONSE_SCHEMA_OUTCOME: Dict[str, Any] = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"demandado\": {\"type\": \"string\", \"enum\": [\"xxx\", \"contrario\"]},\n",
    "        \"resultado\": {\"type\": \"string\", \"enum\": [\"favorable\", \"desfavorable\"]},\n",
    "    },\n",
    "    \"required\": [\"demandado\", \"resultado\"]\n",
    "}\n",
    "\n",
    "# -------------------- Llamada a Vertex (reutilizable) --------------------\n",
    "def vertex_generate_json(prompt: str,\n",
    "                         model_id: str = MODEL_ID,\n",
    "                         temp_primary: float = 0.0,\n",
    "                         temp_fallback: float = 0.2,\n",
    "                         tok_primary: int = 2048,\n",
    "                         tok_fallback: int = 3072) -> str:\n",
    "    model = GenerativeModel(model_id)\n",
    "    last_err = None\n",
    "    # 1) JSON mime con temp baja\n",
    "    try:\n",
    "        cfg = GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            temperature=temp_primary,\n",
    "            max_output_tokens=tok_primary,\n",
    "        )\n",
    "        r = model.generate_content([prompt], generation_config=cfg)\n",
    "        raw = (r.text or \"\").strip()\n",
    "        if raw:\n",
    "            return raw\n",
    "    except Exception as e1:\n",
    "        last_err = e1\n",
    "    # 2) JSON mime fallback\n",
    "    try:\n",
    "        cfg2 = GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            temperature=temp_fallback,\n",
    "            max_output_tokens=tok_fallback,\n",
    "        )\n",
    "        r2 = model.generate_content([prompt], generation_config=cfg2)\n",
    "        raw2 = (r2.text or \"\").strip()\n",
    "        if raw2:\n",
    "            return raw2\n",
    "    except Exception as e2:\n",
    "        last_err = e2\n",
    "    # 3) Libre\n",
    "    r3 = model.generate_content(\n",
    "        [prompt],\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temp_fallback,\n",
    "            max_output_tokens=tok_fallback\n",
    "        )\n",
    "    )\n",
    "    raw3 = (r3.text or \"\").strip()\n",
    "    if raw3:\n",
    "        return raw3\n",
    "    raise RuntimeError(f\"Vertex devolvi√≥ vac√≠o: {last_err if last_err else 'sin detalle'}\")\n",
    "\n",
    "def vertex_generate_json_with_schema(prompt: str, schema: Dict[str, Any],\n",
    "                                     model_id: str = MODEL_ID,\n",
    "                                     temp: float = 0.0,\n",
    "                                     max_tokens: int = 2048) -> str:\n",
    "    model = GenerativeModel(model_id)\n",
    "    r = model.generate_content(\n",
    "        [prompt],\n",
    "        generation_config=GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            temperature=temp,\n",
    "            max_output_tokens=max_tokens\n",
    "        )\n",
    "    )\n",
    "    return (r.text or \"\").strip()\n",
    "\n",
    "# -------------------- Parser gen√©rico --------------------\n",
    "def extract_first_json_value(s: str) -> Optional[str]:\n",
    "    if not s: return None\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'^```(?:json)?\\s*', '', s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    s = re.sub(r'\\s*```$', '', s, flags=re.IGNORECASE)\n",
    "    starts = [i for i, ch in enumerate(s) if ch in \"{[\"]\n",
    "    if not starts: return None\n",
    "    start = starts[0]\n",
    "    open_ch = s[start]; close_ch = \"}\" if open_ch == \"{\" else \"]\"\n",
    "    depth = 0\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "        if ch == open_ch: depth += 1\n",
    "        elif ch == close_ch:\n",
    "            depth -= 1\n",
    "            if depth == 0: return s[start:i+1]\n",
    "    return None\n",
    "\n",
    "# --- Parse motivos (por si lo necesitas en el merged) ---\n",
    "def parsear_motivos_desde_obj(obj) -> List[Tuple[str, float]]:\n",
    "    arr = None\n",
    "    if isinstance(obj, dict):\n",
    "        if \"motivos\" in obj: arr = obj[\"motivos\"]\n",
    "    elif isinstance(obj, list):\n",
    "        arr = obj\n",
    "    out = []\n",
    "    if not isinstance(arr, list): return out\n",
    "    for item in arr:\n",
    "        if isinstance(item, str):\n",
    "            canon = canonizar_motivo(item)\n",
    "            if canon: out.append((canon, 1.0))\n",
    "        elif isinstance(item, dict):\n",
    "            m = item.get(\"motivo\"); c = item.get(\"confianza\", 1.0)\n",
    "            canon = canonizar_motivo(m) if m else None\n",
    "            if canon:\n",
    "                try: c = float(c)\n",
    "                except: c = 1.0\n",
    "                out.append((canon, max(0.0, min(1.0, c))))\n",
    "    return out\n",
    "\n",
    "# -------------------- Clasificaci√≥n OUTCOME por sentencia --------------------\n",
    "def clasificar_outcome_fulltext(nombre_archivo: str, texto: str) -> Tuple[str, str]:\n",
    "    prompt = build_prompt_outcome(texto)\n",
    "    # 1) Intento con schema\n",
    "    try:\n",
    "        raw = vertex_generate_json_with_schema(prompt, RESPONSE_SCHEMA_OUTCOME,\n",
    "                                               model_id=MODEL_ID, temp=0.0, max_tokens=1024)\n",
    "    except Exception:\n",
    "        # 2) Fallbacks\n",
    "        raw = vertex_generate_json(prompt, model_id=MODEL_ID, temp_primary=0.0, temp_fallback=0.2)\n",
    "    # LOG completo\n",
    "    log_block(LOG_PATH, f\"{nombre_archivo} | OUTCOME\", prompt, raw)\n",
    "\n",
    "    frag_json = extract_first_json_value(raw)\n",
    "    ui_json_to_show = frag_json or \"{}\"\n",
    "    ui.show(archivo=nombre_archivo, fase=\"Inferencia OUTCOME completada\",\n",
    "            ultimo_json=ui_json_to_show, ultimo_raw=raw, progress=1.0)\n",
    "\n",
    "    if not frag_json:\n",
    "        return (\"contrario\", \"desfavorable\")  # fallback prudente\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(frag_json)\n",
    "        demandado = str(obj.get(\"demandado\", \"\")).strip().lower()\n",
    "        resultado = str(obj.get(\"resultado\", \"\")).strip().lower()\n",
    "        if demandado not in (\"eos\", \"contrario\"):\n",
    "            demandado = \"contrario\"\n",
    "        if resultado not in (\"favorable\", \"desfavorable\"):\n",
    "            resultado = \"desfavorable\"\n",
    "        return (demandado, resultado)\n",
    "    except Exception:\n",
    "        return (\"contrario\", \"desfavorable\")\n",
    "\n",
    "# -------------------- MAIN: lee de GCS, genera CSV en GCS y fusiona --------------------\n",
    "def main(prefix: Optional[str] = None):\n",
    "   \n",
    "    # Inicializa Vertex\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # 1) Enumerar sentencias .txt desde GCS\n",
    "    txt_blobs = gcs_list_txt(SRC_BUCKET, prefix=prefix)\n",
    "    if not txt_blobs:\n",
    "        print(f\"‚ö†Ô∏è No se encontraron .txt en gs://{SRC_BUCKET}/{prefix or ''}\")\n",
    "        return\n",
    "\n",
    "    total = len(txt_blobs)\n",
    "    rows_outcome = []  # acumularemos aqu√≠ (nombre, demandado, resultado)\n",
    "\n",
    "    for idx, blob_name in enumerate(txt_blobs, start=1):\n",
    "        # Nombre de archivo sin extensi√≥n (para la clave 'nombre')\n",
    "        base = os.path.basename(blob_name)\n",
    "        nombre_sin_ext = os.path.splitext(base)[0]\n",
    "\n",
    "        ui.show(archivo=base, fase=f\"Procesando OUTCOME {idx}/{total}\",\n",
    "                detalle=f\"Descargando texto desde gs://{SRC_BUCKET}/{blob_name}‚Ä¶\",\n",
    "                progress=(idx-1)/total)\n",
    "\n",
    "        try:\n",
    "            texto = gcs_read_text(SRC_BUCKET, blob_name)\n",
    "        except Exception as e:\n",
    "            ui.show(archivo=base, fase=f\"ERROR descarga {idx}/{total}\",\n",
    "                    detalle=f\"No se pudo leer el blob: {e}\", progress=(idx-1)/total)\n",
    "            continue\n",
    "\n",
    "        demandado, resultado = clasificar_outcome_fulltext(base, texto)\n",
    "        rows_outcome.append((nombre_sin_ext, demandado, resultado))\n",
    "\n",
    "        ui.show(archivo=base, fase=f\"Clasificado OUTCOME {idx}/{total}\",\n",
    "                detalle=f\"{nombre_sin_ext} ‚Üí demandado={demandado}, resultado={resultado}\",\n",
    "                progress=idx/total)\n",
    "\n",
    "    # 2) Crear DataFrame y subir CSV de resultados al bucket destino\n",
    "    df_resultado = pd.DataFrame(rows_outcome, columns=[\"nombre\", \"demandado\", \"resultado\"])\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", newline=\"\", suffix=\".csv\", delete=False, encoding=\"utf-8\") as tmpf:\n",
    "        df_resultado.to_csv(tmpf.name, index=False, encoding=\"utf-8\")\n",
    "        gcs_upload_file(DST_BUCKET, GCS_OUT_RES, tmpf.name, content_type=\"text/csv\")\n",
    "\n",
    "    # 3) Intentar fusionar con CSV de motivos en el mismo bucket destino\n",
    "    merged_done = False\n",
    "    if gcs_blob_exists(DST_BUCKET, GCS_MOTIVOS_IN):\n",
    "        # Descargar motivos a temp y leer\n",
    "        tmp_motivos = gcs_download_to_temp(DST_BUCKET, GCS_MOTIVOS_IN)\n",
    "        df_motivos = pd.read_csv(tmp_motivos)\n",
    "\n",
    "        merged = pd.merge(df_motivos, df_resultado, on=\"nombre\", how=\"outer\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode=\"w\", newline=\"\", suffix=\".csv\", delete=False, encoding=\"utf-8\") as tmpm:\n",
    "            merged.to_csv(tmpm.name, index=False, encoding=\"utf-8\")\n",
    "            gcs_upload_file(DST_BUCKET, GCS_OUT_MERGED, tmpm.name, content_type=\"text/csv\")\n",
    "        merged_done = True\n",
    "\n",
    "    # 4) Mensaje final\n",
    "    detalle = f\"CSV OUTCOME ‚Üí gs://{DST_BUCKET}/{GCS_OUT_RES}\"\n",
    "    if merged_done:\n",
    "        detalle += f\"\\nCSV fusionado ‚Üí gs://{DST_BUCKET}/{GCS_OUT_MERGED}\"\n",
    "    else:\n",
    "        detalle += f\"\\n‚ö†Ô∏è No se encontr√≥ gs://{DST_BUCKET}/{GCS_MOTIVOS_IN}. Se omite la fusi√≥n.\"\n",
    "    ui.show(archivo=\"(todos)\", fase=\"Completado\", detalle=detalle, progress=1.0)\n",
    "    print(detalle)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main(prefix=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
