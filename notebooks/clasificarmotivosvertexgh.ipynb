{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939a8ae-1625-463b-998a-992ffe6275cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \".json\"\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview import rag\n",
    "\n",
    "vertexai.init(project=\"proyecto\", location=\"us-east4\")\n",
    "print(list(rag.list_corpora()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e985552-cb55-4bd1-b34d-a6af11725e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4b709-3310-4208-8521-e155da2ca5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Clasificaci√≥n de sentencias (texto completo) con Vertex AI (Gemini 2.0)\n",
    "# - Fuente: GCS bucket \n",
    "# - Salida: gs:///sentencias_motivos_vertex.csv (merge incremental)\n",
    "# =========================================================\n",
    "import os, re, json, time, csv, unicodedata, textwrap, tempfile\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# ---- GCS ----\n",
    "from google.cloud import storage\n",
    "\n",
    "# ---- Vertex AI ----\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "PROJECT_ID = \"project\"\n",
    "# Gemini 2.0 suele estar en us-central1; cambia si lo tienes en otra regi√≥n.\n",
    "LOCATION   = \"us-central1\"\n",
    "MODEL_ID   = \"gemini-2.0-flash\"  # o \"gemini-2.0-pro\"\n",
    "\n",
    "SRC_BUCKET = \"bucket\"   # de aqu√≠ leemos los .txt\n",
    "DST_BUCKET = \"bucket2\"        # aqu√≠ subimos el CSV\n",
    "DST_BLOB_CSV = \"sentencias_motivos_vertex.csv\"\n",
    "\n",
    "LOG_PATH   = \"/dataset/log_vertex.txt\"\n",
    "\n",
    "# Cat√°logo fijo\n",
    "MOTIVOS = [\n",
    "    \"acreditaci√≥n de la deuda\",\n",
    "    \"usura\",\n",
    "    \"abusividad del clausulado\",\n",
    "    \"transparencia\",\n",
    "    \"legitimaci√≥n activa\",\n",
    "    \"prescripci√≥n\",\n",
    "    \"validez de la firma\",\n",
    "    \"legitimaci√≥n pasiva\",\n",
    "    \"requerimiento previo derecho al honor\",\n",
    "]\n",
    "\n",
    "# UI\n",
    "VERBOSE_UI = True\n",
    "UI_WRAP = 110\n",
    "RAW_UI_MAX = 2500  # pon None para ver todo el raw\n",
    "\n",
    "# -------------------- UI din√°mica --------------------\n",
    "def _bar(p: float, width: int = 28) -> str:\n",
    "    p = min(max(p, 0.0), 1.0)\n",
    "    filled = int(round(p * width))\n",
    "    return \"‚ñà\" * filled + \"‚ñë\" * (width - filled)\n",
    "\n",
    "class LiveUI:\n",
    "    def __init__(self, enabled=True): self.enabled = enabled\n",
    "    def _truncate(self, s, max_chars=1200): s = s or \"\";  return (s[:max_chars] + \" ‚Ä¶[+trunc]\") if len(s) > max_chars else s\n",
    "    def show(self, archivo, fase, detalle=None, ultimo_json=None, ultimo_raw=None, progress=None):\n",
    "        if not self.enabled: return\n",
    "        clear_output(wait=True)\n",
    "        lines = []\n",
    "        lines.append(f\"**üìÑ Archivo:** `{archivo}`\")\n",
    "        lines.append(f\"**‚öôÔ∏è Fase:** {fase}\")\n",
    "        if progress is not None:\n",
    "            percent = int(progress * 100)\n",
    "            lines.append(f\"**Progreso:** `{percent:>3d}%`  `{_bar(progress)}`\")\n",
    "        if detalle:\n",
    "            lines.append(f\"**‚ÑπÔ∏è** {detalle}\")\n",
    "        if ultimo_json:\n",
    "            wrapped = \"\\n\".join(textwrap.wrap(self._truncate(ultimo_json, 2000), width=UI_WRAP))\n",
    "            lines.append(\"**üß† Respuesta (JSON limpio):**\\n\\n```json\\n\" + wrapped + \"\\n```\")\n",
    "        if ultimo_raw is not None:\n",
    "            show = ultimo_raw if RAW_UI_MAX is None or len(ultimo_raw) <= RAW_UI_MAX else (ultimo_raw[:RAW_UI_MAX] + \" ‚Ä¶[+trunc]\")\n",
    "            lines.append(\"**üìú Respuesta LITERAL del modelo:**\\n\\n```\\n\" + show + \"\\n```\")\n",
    "        display(Markdown(\"\\n\\n\".join(lines)))\n",
    "\n",
    "ui = LiveUI(enabled=VERBOSE_UI)\n",
    "\n",
    "# -------------------- Utilidades --------------------\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\\n\", \"\\n\", re.sub(r\"[ \\t]{2,}\", \" \", s or \"\")).strip()\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = _strip_accents(s)\n",
    "    repl = {\n",
    "        \"clausulas\": \"clausulado\",\n",
    "        \"clausula\": \"clausulado\",\n",
    "        \"abusivas\": \"abusividad\",\n",
    "        \"abusiva\": \"abusividad\",\n",
    "        \"transparente\": \"transparencia\",\n",
    "        \"transparencia material\": \"transparencia\",\n",
    "        \"legitimacion activa de la parte actora\": \"legitimacion activa\",\n",
    "        \"legitimacion pasiva de la parte demandada\": \"legitimacion pasiva\",\n",
    "        \"validez de la firma digital\": \"validez de la firma\",\n",
    "        \"requerimiento previo\": \"requerimiento previo derecho al honor\",\n",
    "        \"derecho al honor\": \"requerimiento previo derecho al honor\",\n",
    "    }\n",
    "    for k, v in repl.items():\n",
    "        if k in s: s = v\n",
    "    return s\n",
    "\n",
    "_CANON_BY_NORM = { _norm_text(m): m for m in MOTIVOS }\n",
    "\n",
    "def canonizar_motivo(m: str) -> Optional[str]:\n",
    "    if not isinstance(m, str) or not m.strip(): return None\n",
    "    n = _norm_text(m)\n",
    "    if n in _CANON_BY_NORM: return _CANON_BY_NORM[n]\n",
    "    for norm_key, canon in _CANON_BY_NORM.items():\n",
    "        if norm_key in n: return canon\n",
    "    return None\n",
    "\n",
    "def log_block(path, header, prompt_full, response_full):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{ts} | {header}\\n\")\n",
    "        f.write(\"--- PROMPT START ---\\n\")\n",
    "        f.write((prompt_full or \"\") + \"\\n\")\n",
    "        f.write(\"--- PROMPT END ---\\n\")\n",
    "        f.write(\"--- RESPONSE START ---\\n\")\n",
    "        f.write((response_full or \"\") + \"\\n\")\n",
    "        f.write(\"--- RESPONSE END ---\\n\")\n",
    "\n",
    "# -------------------- GCS helpers --------------------\n",
    "_storage = storage.Client()\n",
    "\n",
    "def gcs_list_txt(bucket_name: str, prefix: Optional[str] = None) -> List[str]:\n",
    "    bucket = _storage.bucket(bucket_name)\n",
    "    blobs = _storage.list_blobs(bucket, prefix=prefix)\n",
    "    names = [b.name for b in blobs if b.name.lower().endswith(\".txt\")]\n",
    "    names.sort()\n",
    "    return names\n",
    "\n",
    "def gcs_read_text(bucket_name: str, blob_name: str, encoding: str = \"utf-8\") -> str:\n",
    "    bucket = _storage.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    return blob.download_as_text(encoding=encoding)\n",
    "\n",
    "def gcs_blob_exists(bucket_name: str, blob_name: str) -> bool:\n",
    "    bucket = _storage.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    return blob.exists()\n",
    "\n",
    "def gcs_upload_file(bucket_name: str, blob_name: str, local_path: str, content_type: Optional[str] = None):\n",
    "    bucket = _storage.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(local_path, content_type=content_type)\n",
    "\n",
    "def gcs_download_to_temp(bucket_name: str, blob_name: str) -> str:\n",
    "    bucket = _storage.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    fd, tmp_path = tempfile.mkstemp(prefix=\"gcs_\", suffix=os.path.splitext(blob_name)[1])\n",
    "    os.close(fd)\n",
    "    blob.download_to_filename(tmp_path)\n",
    "    return tmp_path\n",
    "\n",
    "# -------------------- Prompt + Schema --------------------\n",
    "def build_prompt_fulltext(texto: str) -> str:\n",
    "    return f\"\"\"Eres un clasificador jur√≠dico estricto.\n",
    "Analiza la sentencia (texto completo) en espa√±ol y devuelve EXCLUSIVAMENTE un JSON con la clave \"motivos\".\n",
    "Cada motivo debe pertenecer EXACTAMENTE a este cat√°logo y a ninguno m√°s:\n",
    "\n",
    "{json.dumps(MOTIVOS, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Formato preferido (admite tambi√©n lista simple):\n",
    "{{\n",
    "  \"motivos\": [\n",
    "    {{\"motivo\": \"xxx\", \"confianza\": 0.0 a 1.0}},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Reglas MUY IMPORTANTES:\n",
    "- Responde en JSON V√ÅLIDO (RFC 8259) y NADA M√ÅS.\n",
    "- Si NO hay motivos del cat√°logo, devuelve EXACTAMENTE: {{\"motivos\":[]}}.\n",
    "- Devuelve nombres EXACTOS del cat√°logo; si dudas, elige el m√°s cercano del cat√°logo.\n",
    "- M√°ximo 4 motivos relevantes.\n",
    "\n",
    "TEXTO COMPLETO:\n",
    "\\\"\\\"\\\"{normalize_ws(texto)}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "RESPONSE_SCHEMA: Dict[str, Any] = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"motivos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"oneOf\": [\n",
    "                    {\"type\": \"string\", \"enum\": MOTIVOS},\n",
    "                    {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"motivo\": {\"type\": \"string\", \"enum\": MOTIVOS},\n",
    "                            \"confianza\": {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\"motivo\"]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"motivos\"]\n",
    "}\n",
    "\n",
    "# -------------------- Llamada a Vertex --------------------\n",
    "def vertex_generate_json(prompt: str,\n",
    "                         model_id: str = MODEL_ID,\n",
    "                         temp_primary: float = 0.0,\n",
    "                         temp_fallback: float = 0.2,\n",
    "                         tok_primary: int = 2048,\n",
    "                         tok_fallback: int = 3072) -> str:\n",
    "    model = GenerativeModel(model_id)\n",
    "    last_err = None\n",
    "    # 1) Estricto con schema\n",
    "    try:\n",
    "        cfg = GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=RESPONSE_SCHEMA,\n",
    "            temperature=temp_primary,\n",
    "            max_output_tokens=tok_primary,\n",
    "        )\n",
    "        r = model.generate_content([prompt], generation_config=cfg)\n",
    "        raw = (r.text or \"\").strip()\n",
    "        if raw:\n",
    "            return raw\n",
    "    except Exception as e1:\n",
    "        last_err = e1\n",
    "    # 2) JSON mime sin schema\n",
    "    try:\n",
    "        cfg2 = GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            temperature=temp_fallback,\n",
    "            max_output_tokens=tok_fallback,\n",
    "        )\n",
    "        r2 = model.generate_content([prompt], generation_config=cfg2)\n",
    "        raw2 = (r2.text or \"\").strip()\n",
    "        if raw2:\n",
    "            return raw2\n",
    "    except Exception as e2:\n",
    "        last_err = e2\n",
    "    # 3) Libre\n",
    "    r3 = model.generate_content(\n",
    "        [prompt],\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temp_fallback,\n",
    "            max_output_tokens=tok_fallback\n",
    "        )\n",
    "    )\n",
    "    raw3 = (r3.text or \"\").strip()\n",
    "    if raw3:\n",
    "        return raw3\n",
    "    raise RuntimeError(f\"Vertex devolvi√≥ vac√≠o: {last_err if last_err else 'sin detalle'}\")\n",
    "\n",
    "# -------------------- Parser + normalizaci√≥n --------------------\n",
    "def extract_first_json_value(s: str) -> Optional[str]:\n",
    "    if not s: return None\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'^```(?:json)?\\s*', '', s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    s = re.sub(r'\\s*```$', '', s, flags=re.IGNORECASE)\n",
    "    starts = [i for i, ch in enumerate(s) if ch in \"{[\"]\n",
    "    if not starts: return None\n",
    "    start = starts[0]\n",
    "    open_ch = s[start]; close_ch = \"}\" if open_ch == \"{\" else \"]\"\n",
    "    depth = 0\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "        if ch == open_ch: depth += 1\n",
    "        elif ch == close_ch:\n",
    "            depth -= 1\n",
    "            if depth == 0: return s[start:i+1]\n",
    "    return None\n",
    "\n",
    "def parsear_motivos_desde_obj(obj) -> List[Tuple[str, float]]:\n",
    "    arr = None\n",
    "    if isinstance(obj, dict):\n",
    "        if \"motivos\" in obj: arr = obj[\"motivos\"]\n",
    "    elif isinstance(obj, list):\n",
    "        arr = obj\n",
    "    out = []\n",
    "    if not isinstance(arr, list): return out\n",
    "    for item in arr:\n",
    "        if isinstance(item, str):\n",
    "            canon = canonizar_motivo(item)\n",
    "            if canon: out.append((canon, 1.0))\n",
    "        elif isinstance(item, dict):\n",
    "            m = item.get(\"motivo\"); c = item.get(\"confianza\", 1.0)\n",
    "            canon = canonizar_motivo(m) if m else None\n",
    "            if canon:\n",
    "                try: c = float(c)\n",
    "                except: c = 1.0\n",
    "                out.append((canon, max(0.0, min(1.0, c))))\n",
    "    return out\n",
    "\n",
    "# -------------------- Pipeline por sentencia --------------------\n",
    "def clasificar_sentencia_fulltext(nombre_archivo: str, texto: str) -> List[str]:\n",
    "    prompt = build_prompt_fulltext(texto)\n",
    "    raw = vertex_generate_json(prompt)\n",
    "    log_block(LOG_PATH, f\"{nombre_archivo} | FULLTEXT\", prompt, raw)\n",
    "\n",
    "    frag_json = extract_first_json_value(raw)\n",
    "    ui_json_to_show = frag_json or \"{}\"\n",
    "    ui.show(archivo=nombre_archivo, fase=\"Inferencia completada\",\n",
    "            ultimo_json=ui_json_to_show, ultimo_raw=raw, progress=1.0)\n",
    "\n",
    "    if not frag_json:\n",
    "        return []\n",
    "    try:\n",
    "        obj = json.loads(frag_json)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    pares = parsear_motivos_desde_obj(obj)\n",
    "    seen, motivos = set(), []\n",
    "    for m, c in sorted(pares, key=lambda x: -x[1]):\n",
    "        if m not in seen:\n",
    "            motivos.append(m); seen.add(m)\n",
    "        if len(motivos) == 4: break\n",
    "    return motivos\n",
    "\n",
    "# -------------------- MAIN (lee GCS, sube CSV a GCS con merge incremental) --------------------\n",
    "def run_from_gcs(prefix: Optional[str] = None):\n",
    "    # Vertex\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # 1) Listado de .txt en el bucket origen\n",
    "    txt_blobs = gcs_list_txt(SRC_BUCKET, prefix=prefix)\n",
    "    if not txt_blobs:\n",
    "        print(f\"‚ö†Ô∏è No se encontraron .txt en gs://{SRC_BUCKET}/{prefix or ''}\")\n",
    "        return\n",
    "\n",
    "    total = len(txt_blobs)\n",
    "    rows = []  # (nombre, motivo1..motivo4)\n",
    "\n",
    "    for idx, blob_name in enumerate(txt_blobs, start=1):\n",
    "        base = os.path.basename(blob_name)\n",
    "        nombre_sin_ext = os.path.splitext(base)[0]\n",
    "\n",
    "        ui.show(archivo=base, fase=f\"Procesando {idx}/{total}\",\n",
    "                detalle=f\"Leyendo gs://{SRC_BUCKET}/{blob_name}‚Ä¶\", progress=(idx-1)/total)\n",
    "\n",
    "        try:\n",
    "            texto = gcs_read_text(SRC_BUCKET, blob_name)\n",
    "        except Exception as e:\n",
    "            ui.show(archivo=base, fase=f\"ERROR lectura {idx}/{total}\",\n",
    "                    detalle=f\"No se pudo leer el blob: {e}\", progress=(idx-1)/total)\n",
    "            continue\n",
    "\n",
    "        motivos = clasificar_sentencia_fulltext(base, texto)\n",
    "        motivos = (motivos + [\"null\", \"null\", \"null\", \"null\"])[:4]\n",
    "        rows.append((nombre_sin_ext, *motivos))\n",
    "\n",
    "        ui.show(archivo=base, fase=f\"Clasificada {idx}/{total}\",\n",
    "                detalle=f\"{nombre_sin_ext} ‚Üí {motivos}\", progress=idx/total)\n",
    "\n",
    "    # 2) DataFrame con nuevas filas\n",
    "    df_new = pd.DataFrame(rows, columns=[\"nombre\", \"motivo1\", \"motivo2\", \"motivo3\", \"motivo4\"])\n",
    "\n",
    "    # 3) Si existe CSV previo en destino, hacemos merge incremental por 'nombre'\n",
    "    if gcs_blob_exists(DST_BUCKET, DST_BLOB_CSV):\n",
    "        tmp_old = gcs_download_to_temp(DST_BUCKET, DST_BLOB_CSV)\n",
    "        try:\n",
    "            df_old = pd.read_csv(tmp_old)\n",
    "        except Exception:\n",
    "            df_old = pd.DataFrame(columns=[\"nombre\", \"motivo1\", \"motivo2\", \"motivo3\", \"motivo4\"])\n",
    "        # concat y dedup por 'nombre' (prioriza nuevas filas)\n",
    "        df_merged = pd.concat([df_old[~df_old[\"nombre\"].isin(df_new[\"nombre\"])], df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_merged = df_new\n",
    "\n",
    "    # 4) Subir CSV final al bucket destino\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", newline=\"\", suffix=\".csv\", delete=False, encoding=\"utf-8\") as tmpf:\n",
    "        df_merged.to_csv(tmpf.name, index=False, encoding=\"utf-8\")\n",
    "        gcs_upload_file(DST_BUCKET, DST_BLOB_CSV, tmpf.name, content_type=\"text/csv\")\n",
    "\n",
    "    ui.show(archivo=\"(todos)\", fase=\"Completado\",\n",
    "            detalle=f\"CSV final ‚Üí gs://{DST_BUCKET}/{DST_BLOB_CSV}\", progress=1.0)\n",
    "    print(f\"CSV final: gs://{DST_BUCKET}/{DST_BLOB_CSV}\")\n",
    "\n",
    "run_from_gcs(prefix=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
